<%= form_for(artwork) do |f| %>
  <% if artwork.errors.any? %>
    <div id="error_explanation">
      <h2><%= pluralize(artwork.errors.count, "error") %> prohibited this artwork from being saved:</h2>

      <ul>
      <% artwork.errors.full_messages.each do |message| %>
        <li><%= message %></li>
      <% end %>
      </ul>
    </div>
  <% end %>

  <div class="field">
    <%= f.label :source_file %>
    <%= f.file_field :source_file %>
  </div>

  <div class="field">
    <%= f.label :style_file %>
    <%= f.file_field :style_file %>
  </div>
  
  <div class="field">
    <%= f.label :ext_arg %>
    <%= f.text_field :ext_arg, :size=>80 %>
  </div>

  <div class="actions">
    <%= f.submit %>
  </div>
<% end %>

<h1>Options</h1>
<pre>
Options:
-image_size: Maximum side length (in pixels) of of the generated image. Default is 512.
-style_blend_weights: The weight for blending the style of multiple style images, as a comma-separated list, such as -style_blend_weights 3,7. By default all style images are equally weighted.
-gpu: Zero-indexed ID of the GPU to use; for CPU mode set -gpu to -1.

Optimization options:
-content_weight: How much to weight the content reconstruction term. Default is 5e0.
-style_weight: How much to weight the style reconstruction term. Default is 1e2.
-tv_weight: Weight of total-variation (TV) regularization; this helps to smooth the image. Default is 1e-3. Set to 0 to disable TV regularization.
-num_iterations: Default is 1000.
-init: Method for generating the generated image; one of random or image. Default is random which uses a noise initialization as in the paper; image initializes with the content image.
-optimizer: The optimization algorithm to use; either lbfgs or adam; default is lbfgs. L-BFGS tends to give better results, but uses more memory. Switching to ADAM will reduce memory usage; when using ADAM you will probably need to play with other parameters to get good results, especially the style weight, content weight, and learning rate; you may also want to normalize gradients when using ADAM.
-learning_rate: Learning rate to use with the ADAM optimizer. Default is 1e1.
-normalize_gradients: If this flag is present, style and content gradients from each layer will be L1 normalized. Idea from andersbll/neural_artistic_style.

Output options:
-output_image: Name of the output image. Default is out.png.
-print_iter: Print progress every print_iter iterations. Set to 0 to disable printing.
-save_iter: Save the image every save_iter iterations. Set to 0 to disable saving intermediate results.
Layer options:

-content_layers: Comma-separated list of layer names to use for content reconstruction. Default is relu4_2.
-style_layers: Comma-separated list of layer names to use for style reconstruction. Default is relu1_1,relu2_1,relu3_1,relu4_1,relu5_1.

Other options:
-style_scale: Scale at which to extract features from the style image. Default is 1.0.
-original_colors: If you set this to 1, then the output image will keep the colors of the content image.
-proto_file: Path to the deploy.txt file for the VGG Caffe model.
-model_file: Path to the .caffemodel file for the VGG Caffe model. Default is the original VGG-19 model; you can also try the normalized VGG-19 model used in the paper.
-pooling: The type of pooling layers to use; one of max or avg. Default is max. The VGG-19 models uses max pooling layers, but the paper mentions that replacing these layers with average pooling layers can improve the results. I haven't been able to get good results using average pooling, but the option is here.
-backend: nn, cudnn, or clnn. Default is nn. cudnn requires cudnn.torch and may reduce memory usage. clnn requires cltorch and clnn
-cudnn_autotune: When using the cuDNN backend, pass this flag to use the built-in cuDNN autotuner to select the best convolution algorithms for your architecture. This will make the first iteration a bit slower and can take a bit more memory, but may significantly speed up the cuDNN backend.
</pre>